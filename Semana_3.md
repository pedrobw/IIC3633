## Semana 3

### [Evaluating recommendation systems (2011). Shani, G. & Gunawardana, A.](http://www.bgu.ac.il/~shanigu/Publications/EvaluationMetrics.17.pdf)

Este extenso documento abarca toda una serie de buenas prácticas a seguir a la hora de proponer un nuevo sistema recomendador y compararlo con los existentes. En primer lugar, se discuten cómo debe ser llevado cada experimento, considerando, entre otras cosas, un conjunto de hipótesis, variables controladoras y poder de generalización que tiene, si el experimento es _offline_ (con datos ya recaudados) u _online_ (en directa observación de la interacción del usuario con el sistema) con sus respectivas ventajas, desventajas y limitaciones, y la calidad de las conclusiones que es posible extraer, utilizando para ello intervalos de confianza y sin olvidar los inconvenientes que surgen de las leyes de probabilidad al tomar conclusiones simultáneamente.

Finalmente, se proponen una serie de propiedades a testear en un sistema recomendador, incluyendo la exactitud (que vendría siendo definida como la reducción del error, usualmente medida con el RMSE y el MAE, o sus versiones normalizadas o promediadas), la precisión (que considera los verdaderos/falsos positivos/negativos y sus generalizaciones a curvas ROC y de _precision/recall_), medidas de rankeo (como la NDPM que solo toma en cuenta la predicción de signo, el R-score que considera un concepto de utilidad y otras variaciones utilitaristas como el DCG, NDCG y el ARHR), el cubrimiento o _coverage_  de ítems (que indica la proporción de elementos del data set que el sistema puede recomendar y que incluye medidas como la diversidad utilizando el coeficiente de Gini o la entropía de Shannon) y de usuarios (que indica la cantidad o proporción de usuarios a los que el sistema puede recomendar ítems), la resolución del problema de _cold start_ (que ya discutimos en otra entrada), la confianza (en el sentido estadístico, normalmente utilizando intervalos o rangos), la confianza (del usuario en el sistema), la novedad (entendida como la capacidad de generar predicciones de elementos que el usuario no ha consumido y medida de maneras astutas ya sea considerando la popularidad o ignorando ítems pasados), la serendipidad (entendida como la capacidad de generar ítems que el usuario no habría conocido de otro modo, es decir, en cierta forma el nivel de sorpresa que generan las recomendaciones), la diversidad (de las recomendaciones), la utilidad (monetaria del servicio), el riesgo (en sistemas de incertidumbre en las que el riesgo es indeseable, por ejemplo, en la recomendación de opciones de inversión, y usualmente entendido en términos de la varianza), la robustez (entendida como la estabilidad frente a la presencia de información falsa, pero equivalente como la robustez en términos de optimalidad por oposición a la sensibilidad a los parámetros del modelo), la privacidad (en términos de si el sistema puede ser o no abusado para revelar información personal), la adaptabilidad (como la capacidad del sistema a cambiar rápidamente en función de altos flujos de información o preferencias cambiantes) y la escalabilidad (comprendida en cuanto a la complejidada computacional de ejercer la predicción o recomendación en términos de tiempo y espacio).

Aprecio el hecho de que este _paper_ considerara la evaluación de una manera tan extensa y considerando tantas variables, aunque se nota que podría ser actualizado (habiendo trabajado en privacidad hace un tiempo, sé que hay medidas más interesantes y aceptadas actualmente, por ejemplo), y siento que podría haber explicado mejor las medidas más desconocidas en lugar de enfatizar tanto en las más conocidas como el RMSE y el MAE. 
