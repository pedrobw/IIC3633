## Semana 6

### [Factorization Machines (2010). Rendle, S.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)

En este paper, Rendle introduce el concepto de máquinas de factorización, una nueva técnica de factorización basada en un modelo de optimización que funciona como predictor general (de manera similar a los SVMs) y que es efectivo para estimar de manera confiable parámetros incluso para _datasets_ de baja densidad (es decir, con alta _sparsity_). Además, el autor muestra que la ecuación principal del modelo se puede computar en tiempo que es lineal en los parámetros y, por lo tanto, escalable, que su modelo es capaz de emular las técnicas que eran en ese momento estado del arte (MF, SVD++, PITF, FPMC), y que el modelo funciona en _datasets_ que son _sparse_ en los que SVM falla.

Para explicar el modelo, el autor plantea una base de datos inicial D = {(x<sup>1</sup>,y<sup>1</sup>), (x<sup>2</sup>,y<sup>2</sup>), ..., (x<sup>|D|</sup>,y<sup>|D|</sup>)} de pares que relacionan un _feature_ y<sup>i</sup> que se quiere ser capaz de predecir con un vector de datos x<sup>i</sup> de dimensión n, que en particular para este _paper_, debe ser _sparse_, vale decir, contener 0s en gran parte de sus coordenadas. Para esto, se define m(x), la cantidad de valores no nulos que contiene un vector x, el promedio m<sub>D</sub> de dichos valores, con lo que la _sparsity_ es equivalente a tener m<sub>D</sub> << n. Con esto es posible armar un modelo de predicción que estima y de la manera siguiente:

ŷ(x) := w<sub>0</sub> +  ∑<sub>i=1..n</sub>w<sub>i</sub>x<sub>i</sub> + ∑<sub>i=1..n</sub>∑<sub>j=i+1..n</sub> <v<sub>i</sub>, v<sub>j</sub>>x<sub>i</sub>x<sub>j</sub>

donde los parámatros a estimar son w<sub>0</sub>, w (en R<sup>n</sup>), V (en R<sup>nxk</sup>), y donde w<sub>0</sub> es el sesgo global, w es el vector de pesos de los regresores y V modela la interacción que existe entre los regresores a través del producto interno de la ecuación.

Enseguida, Rendle afirma que para k grande, siempre se puede encontrar W de manera que W=VV<sup>t</sup>, con lo que se expresa bien la interacción, que la interacción a través del producto punto es útil incluso bajo _sparsity_ funciona bien, y lo más importante, que es posible calcular el ŷ(x) de manera eficiente en O(kn) al reescribir las sumas con un poco de manipulación algebraica, que además en supuesto de sparsity, se traduce por cálculo en O(km<sub>D</sub>).

A continuación, el _paper_ continúa explicando como usar estas máquinas de factorización pueden ser usadas para tareas de regresión (predicción a través de ŷ(x)), clasificación binaria (viendo el signo de ŷ(x)) y ranking (según el valor de ŷ(x) para comparar los x). Además, muestra que la resolución de un modelo de optimización es computable de manera eficiente dado que el gradiente es fácil de calcular y muestra que las máquinas de factorización binarias (en cuanto la interacción es de a pares) se pueden extender para integrar interacciones de aridad d al generalizar el producto punto y con similares propiedades de complejidad.

Finalmente, hay dos secciones enteras dedicadas a mostrar las similares y diferencias entre las máquinas de factorización y los otros modelos comúnmente usados. En particular, se muestra acá primero cómo el modelo de SVM es similar en su formulación para un kernel lineal o polinomial y por qué este es incapaz de trabajar tan bien con datos _sparse_, a diferencia de las máquinas de factorización, al reescribirlo bajo el mismo formato que estas. Luego se muestra una comparación similar con otros modelos. Notablemente algo similar ocurre con factorización matricial que hace que no funciones bajo datos _sparse_, se muestra por qué SVD++ se puede escribir en este formato y cómo es limitado en la integración de las interacciones, se muestra la gran similaridad con PITF (que es utilizado para predecir _tags_) al reescribirlo y las sutiles diferencias entre ambos (FM captura otros sesgos y toma las interacciones de manera compartida), al igual que la gran similaridad con FPMC (que en este caso difieren en esa estructura solo en términos de sesgos).

Personalmente, me parece que es un _paper_ muy completo y con una sólida base matemática, mucho más claro que la mayoría de los _papers_ que leímos anteriormente y que deja claras las razones por las que el modelo funciona y es eficiente de calcular. Valoro el que hayan analizado la complejidad de cálculo, puesto que es un factor importante cuando se debe implementar un modelo en código, y además me parece genial su manera de generalizar los métodos de estado del arte con este modelo más general. Lo único que sentí que faltó fue un mayor respaldo práctico basado en algún _dataset_ clásico para demostrar de manera completa la superioridad de la técnica, aunque personalmente prefiero estos _papers_ más teóricos.
